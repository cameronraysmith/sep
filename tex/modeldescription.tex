%!TEX root = ../plos_template.tex

\section{Environments of biological networks as abstract contexts}\label{sec:networkcontext}
Most studies of biological networks study one type of biological variable in isolation. For example, many studies focus on one of metabolic networks, protein-protein interaction networks, signalling networks, gene-regulatory networks, or population and community dynamics in the context of ecological networks. A true biological network involves all of these acting together to produce biological phenomena at all scales. The Systems Biology Graphical Notation (SBGN) supports the ability to express many of these networks within the context of a single formalism \cite{LeNovere2009}, \ref{fig:netsubnetcontext}. Even when the different types of biological variables are combined into a single network, it is impossible to study all variables simultaneously. As a result, it is always the case that a subnetwork is selected for investigation and the remainder of the network is treated as an environment. In \ref{fig:netsubnetcontext} we show the SBGN process form of six simple examples of biological networks. In each case we have selected a subset of variables that form a subnetwork as an example of how one might proceed in the investigation of a particular biological system. Once such a subnetwork is chosen, one implicitly abstracts away the variables that are not part of the subnetwork. This is represented by the abstract influence network (\AI{}) for each simple example on the second row of \ref{fig:netsubnetcontext}. The transformation from SBGN to the \AI{} network is given simply by collapsing the disconnected components of the ancestors of each node in the focal subnetwork into single \AI{} nodes. This results in a bipartite graph that captures the dependencies among the environmental factors as experienced by the subnetwork and nothing more.

This \AI{} graph is precisely equivalent to an undirected hypergraph if one considers each of the \AI{} nodes as a hyperedge containing all of the nodes to which it connects. This is shown in the third row of \ref{fig:netsubnetcontext} for each of the simple examples of the SBGN form of biological networks. Considering all possible hypergraphs of this kind is equivalent to examining all possible environmental dependency structures the subnetwork could be subjected to. Because the \AI{} is fundamental to understanding how subnetworks depend upon their contexts, it is the structure of the \AI{} graph that we refer to as \emph{network architecture} throughout this paper. We note from this perspective, that cycles in the SBGN representation of the biological network do not result in corresponding cycles in the \AI{} graph and vice versa. For instance, in example four of \ref{fig:netsubnetcontext}, there are no cycles in the SBGN representation of the biological network whereas a single cycle exists in the hypergraph representation of the \AI{} graph. Furthermore, in example six, there is a cycle in the SBGN representation, whereas there is no cycle in the hypergraph representation of the \AI{}.

More precisely, the collection of variables comprising the subnetwork under consideration is referred to as $L$. The different subsets, $O$, of biological variables, $L$, making up the hypergraph representation of the \AI{} are each referred to as modules. A biological network architecture (BNA), $\mathcal{G}$, may then be represented by a subset of all possible such modules subject to two conditions (see \refsupp{} \ref{sec:covergenotypespace}). The first represents the fact each variable of the focal subnetwork must be included in at least one module. The second represents the fact that any pair of constraints that are imposed upon overlapping sets of variables must agree on those overlapping variables.  In expressing the latter condition, all of the information present in a collection of lower-order constraints can be expressed as an effective higher-order constraint if any such higher-order constraint exists at all. So, if there is a constraint that is imposed simultaneously upon two distinct variables and another independent constraint imposed upon only the first of the two variables, this situation can be expressed in terms of a single constraint on both of the two variables.

When there is a relatively larger degree of independence in the network context as compared to the subnetwork, it is possible for inconsistency to arise. One canonical example of such inconsistency arises in the study of ferromagnetism via the Ising model on a triangular lattice where so-called \emph{frustration} arises in the couplings among the magnetic dipole moments of three nearest-neighbor atomic spins \cite{Wannier1950,Toulouse1977,Vannimenus1977}. In this example, the underlying lattice or graph represents interactions among the spins of atomic nuclei according to their spatial proximity. As we have described, in our model, the network architectures to which we refer represent the manner in which the network context places constraints upon a subnetwork. Inconsistency is likewise capable of arising if there is a cycle in the hypergraph representing this network architecture.
% \ref{fig:netsubnetcontext}. A different graph representing physical interactions among the elements of the subnetwork may contain cycles without there being any cycle in the graph representing the manner in which this subnetwork is connected to the context within which it is embedded.

\section{Coarse-graining dynamic network states as a generalization of genotype-phenotype maps}\label{sec:genenetworkphenmap}

\ref{fig:expression_concept}A shows a simplified representation of two different biological networks the correlation strengths among whose variables are not known but are to be derived from observation of the levels of the entities corresponding to each variable. For example, in the context of a gene-regulatory network, the amount of a given transcript present in a cell given in terms of the discrete counts obtained via sequence census methods (e.g. RNA-seq) or relative abundance derived from microarray data can be binned into a smaller number of discrete classes by setting a collection of thresholds on the original data set. If only a single threshold is given, then the data can be binned into two classes depending upon whether or not the original measurement surpasses the given threshold in \ref{fig:expression_concept}B.
The time series that results from such observations can be used to infer various statistics that characterize the dynamics of a biological network such as correlations between pairs of variables.
%The statistics associated to any higher-level phenotype that is determined by a particular gene expression pattern should be functions of such statistics.

If a large enough number of thresholds is available to distinguish among all possible counts of the variables under investigation, then this observational protocol becomes complementary to mechanistic models.  There may be several sources for stochasticity in the dynamics including small numbers of the causal molecules and products as well as environmental fluctuations upon which these dynamics are conditioned~\cite{Swain2002,Paulsson2004,Thattai2004,Acar2008a,Lestas2010,Munsky2012,Chalancon2012,Neuert2013,Sanchez2013}. Regardless of the fundamental nature of stochasticity to biological networks, empirically, we observe statistical as opposed to deterministic data, and thus we focus here on stochastic models.
%For our purposes we assume that we are dealing with a stationary stochastic process.
Mathematically, such a model may take the form of a Markov chain whose dynamics are governed by a master equation for probability distributions over molecule counts. For example, in the case of a three variable network, the master equation takes the form
$$
\frac{dP(n_1,n_2,n_3)}{dt} = \sum_{n'_1}\sum_{n'_2}\sum_{n'_3} M^{n_1\,n_2\,n_3}_{n'_1\,n'_2\,n'_3}(k) P(n'_1,n'_2,n'_3)
$$
where $P(n_1,n_2,n_3)$ gives the probability of observing $n_1$, $n_2$, and $n_3$ molecules of each of the three variables respectively and $M(k)$ is a Markov transition rate matrix that depends upon some rate functions $k$ that are determined by the network architecture and the dynamics of the interactions.  The solution to this equation will converge towards a stationary distribution $P_s$ in the limit of long times. Any environmental variable having a characteristic timescale longer than that of the variables in the focal subnetwork would not be sensitive to transients and would only exhibit control over or be influenced by this stationary distribution.

Interactions between variables may be mediated by a coarse-graining over counts of each variable using a function that maps the states represented by vectors of natural numbers into some other variables. For example, if $n_i$ are natural numbers
%, denoting the latter $\mathbb{N}$, then a function $f \colon \mathbb{Z}^+ \rightarrow \{0,1\}$ that
then a function $f$ taking any number less than or equal to some threshold $T$ to $0$ and any number greater than $T$ to $1$ is a very simple example of such a coarse-graining. For this specific form of the coarse-graining function $f$, the coarse-grained stationary probability distribution takes the form
$$
P_s(b_1,b_2,b_3) = \sum_{n_1 \in f^{-1}(b_1)}\sum_{n_2 \in f^{-1}(b_2)}\sum_{n_3 \in f^{-1}(b_3)} P_s(n_1,n_2,n_3),
$$
where $b_1,b_2,b_3 \in \{ 0,1 \}$. It is also possible to consider the case where each variable is coarse-grained according to a different threshold and into a different number of classes.

The most familiar example of such a coarse-graining process in biology is the genotype-phenotype map. The genotype of an organism has a relatively straightforward definition in terms of the sequence of nucleotides comprising its genome. Phenotypes, on the other hand, can be described at different levels of organization~\cite{Dawkins1982,Stadler2001}. The concept of phenotype was initially defined at the level of macroscopically observable physical characteristics such as shape, size, color, and various combinations thereof~\cite{Johannsen1911}. However, since the advent of molecular biology, the lowest level description of phenotype might be considered to be the dynamic phenomenon that can be described by measuring the transcription states of all genes comprising an organism's genome.  These expression levels of subsets of interacting genes determine which enzymes are produced, thus determining the rate at which metabolic reactions proceed.  These reaction rates could then be viewed as constituting the next level of phenotypes.  These in turn determine even higher level phenotypes, ultimately culminating in macroscopically observable ones where the concept of phenotype was originally introduced.
%In order to discuss this notion of phenotypic levels in a more precise way, we develop a formal description of the \gnpm{} that takes into account higher-order correlations among genes as well as evidence that gene expression is either stochastic or is more effectively modeled as such.

% The data derived from a number of samples of a given network can be described using binary sequences as in \ref{fig:expression_concept}C.
A more realistic basis upon which to build phenotypes than this outline of the historical trajectory contains is one that is not limited to genes alone, but includes all entities constituting a biological network. A phenotype must be a function of the levels of, for example, all of the molecular constituents that comprise it over time, even if more information is required to fully specify it.  The aforementioned coarse-grained levels of biological network variables can thus be viewed as collectively determining the lowest level in a hierarchy of abstract phenotypes. In what proceeds, we will assume that we have a finite set $L$ of variables and a finite set $P$ of coarse-grained levels of each of those variables.
%These levels can also be viewed as complementary to promoter states, and, realistically, the number of these is likely to be larger than two \cite{Rieckh2013a}.
Then a possible state of our biological network is represented by a function $e : L \to P$ and coarse-graining a stationary distribution will lead to a probability distribution on the set of all maps, denoted $P^L$, from subnetworks represented by subsets of $L$ to the respective states of the variables that comprise them. We will refer to this more fine-grained generalization of the genotype-phenotype map, where arbitrary biological networks are substituted for genes and arbitrary networks states are substituted for phenotypes, as \gnpm{}.

% \section{Coarse-graining phenotypes}\label{sec:coarsegrainingphenotypes}

% As described in \ref{sec:genenetworkphenmap} it is also possible to consider phenotypes that derive from coarse-graining lower-level phenotypes. Once this is done, one arrives at probability distributions over network modules like that introduced in \ref{sec:probabilitydistributionsonnetworks}. As a result of this, our conclusions that are formulated in terms of a single level of coarse-graining \gnpm{} also apply to coarse-graining over multiple levels at once despite the fact that the parameters of the relevant probabilistic model are likely to be different.

% For a subset of genes $O \subseteq L$, let $\phi_i (O)$ be the set of phenotype values at level $i$, which can be determined from the expression levels of genes in $O$.  Note that $\phi_i (O)$ may be empty if the set $O$ does not contain enough genes to determine the values of any phenotype at level $i$. If $i \le j$, let $\Omega_{ij}(O) : \phi_i(O) \to \phi_j(O)$ be the coarse-graining map which describes how higher level phenotypes are determined from lower level phenotypes. This map is formulated precisely in \refsupp{} \ref{secsupp:coarsegrainingphenotypes}.

% For example, if our lower level phenotypes for a set of genes $O_1 = \{ l_1,l_2,l_3,l_4 \}$ are given by a set of binary sequences, then the projection of these phenotypes down to the set $O_2 = \{l_3,l_4\}$ followed by mapping to the higher level phenotypes $x=\{01,10\}$ and $y=\{11\}$ is equivalent to first mapping to the higher-level phenotypes $X$ and $Y$ and then projecting down to $O_2$ shown by the equivalent paths from the top-left to the bottom-right in \ref{fig:phenotypehierarchy}A.
% Of course, there is an equivalent diagram for the subset $\{ l_1,l_2,l_3 \}$.

\section{Probability distributions over network modules}\label{sec:probabilitydistributionsonnetworks}
Here we describe examples of probability distributions over network modules. A more general presentation is provided in \refsupp{} \ref{secsupp:probabilitydistributionsonnetworks}.

As explained in \ref{sec:networkcontext}, for a given biological subnetwork, the hypergraph representing the dependencies in the network context consists of subsets, $O$, of the variables, $L$, in the subnetwork.
% The states of these are given by functions from these subsets to $P$. If $O \subset L$, let $\expr{}(O)$ denote the set of all possible functions from $O$ to $P$.
% If we consider the case in which we have two variables $L=\{l_1,l_2\}$ and there are two states, $P=\{0,1\}$, then $\expr$ operates on each of the possible subsets of $L$ (i.e. $\{\}$, $\{l_1\}$, $\{l_2\}$, $\{l_1,l_2\}$) to give spaces of functions containing the possible \gnpm{} as exemplified in \ref{fig:efunctor}. For example, $\mathcal{E}(\{l_1,l_2\}) = \{ e^{12}_{00},e^{12}_{01},e^{12}_{10},e^{12}_{11} \}$ where $e^{12}_{01}(l_1) = 0$ and $e^{12}_{01}(l_2) = 1$. As another example, $\mathcal{E}(\{l_1\}) = \{ e^{1}_{0},e^{1}_{1} \}$ where $e^{1}_{0}(l_1)=0$ and $e^{1}_{1}(l_1)=1$.
If we consider the case in which we have two variables $L=\{l_1,l_2\}$ and there are two values, $P=\{0,1\}$. Given two variables and two values, there are four possible assignments of values to variables each of which constitutes a state of the system. We will write the probability of each of these states as $p^{v_1v_2}_{s_1s_2}$ indicating that variable $v_1$ is assigned value $s_1$ and variable $v_2$ is assigned value $s_2$. A probability distribution over the states of the system for $L$ is then given by
% Given a finite set $S$, define $\dist(S)$ to be the set of all probability distributions on $S$. Continuing the example above,
\begin{equation}\label{eq:examplejointdist}
\{p^{12}_{00},p^{12}_{01},p^{12}_{10},p^{12}_{11} \mid p^{12}_{00} \geq 0, p^{12}_{01} \geq 0,p^{12}_{10} \geq 0,p^{12}_{11} \geq 0, p^{12}_{00} + p^{12}_{01} + p^{12}_{10} + p^{12}_{11} = 1 \}.
\end{equation}
This imposes the standard conditions that probabilities are positive and sum to one. If we have the subset of $L$ given by $O = \{l_1\}$ then a probability distribution over its states is given by
\begin{equation}\label{eq:examplemargdist}
\{p^{1}_{0}, p^{1}_{1} \mid p^{1}_{0} \geq 0, p^{1}_{1} \geq 0, p^{1}_{0}+p^{1}_{1} = 1 \}.
\end{equation}
% If $S$ and $S'$ are finite sets, which in our case will usually be sets of \gnpm{} given by $\mathcal{E}(O)$, then $d \in \dist(S)$ and $d' \in \dist(S')$ are probability distributions. If $f \colon S \to S'$
% is an onto map, then $\dist{}(f)$ is defined as marginalization. For example, in case $S=\{ e^{12}_{00}, e^{12}_{01}, e^{12}_{10}, e^{12}_{11} \}$, $S'=\{ e^{1}_{0}, e^{1}_{1} \}$, and $f$ is given by
% \begin{equation}
% \begin{aligned}
% e^{12}_{00} \mapsto e^{1}_{0}\\
% e^{12}_{01} \mapsto e^{1}_{0}\\
% e^{12}_{10} \mapsto e^{1}_{1}\\
% e^{12}_{11} \mapsto e^{1}_{1}
% \end{aligned},
% \end{equation}
% then $\dist{}(f)$ can be represented as a marginalization matrix
In order to be consistent the distribution expressed in \ref{eq:examplejointdist} should be related to that of \ref{eq:examplemargdist} via a marginalization matrix
\begin{equation}
\begin{pmatrix}
p^{1}_{0}\\
p^{1}_{1}
\end{pmatrix} = \begin{pmatrix}
1 & 1 & 0 & 0\\
0 & 0 & 1 & 1
\end{pmatrix}
\begin{pmatrix}
p^{12}_{00}\\
p^{12}_{01}\\
p^{12}_{10}\\
p^{12}_{11}
\end{pmatrix}.
\end{equation}

\section{Compatibility of distributions on \gnpm{}}\label{sec:compatibilityofgpms}
When one has a non-trivial hypergraph, there will typically be more
than one way of obtaining a probability distribution on a set by
marginalizing a distribution on a larger set.  For instance, if we have
a network with three binary variables and two edges, $\{l_1,l_2\}$ and
$\{l_1,l_3\}$, then we can obtain a probability distribution on the
set $\{l_1\}$ either by marginalizing probabilities defined over
$\{l_1,l_2\}$ as was done above or by marginalizing probabilities
defined over $\{l_1,l_3\}$ to obtain
\begin{equation}
 \begin{pmatrix}
  p^{1}_{0}\\
  p^{1}_{1}
 \end{pmatrix} =
 \begin{pmatrix}
  1 & 1 & 0 & 0\\
  0 & 0 & 1 & 1
 \end{pmatrix}
 \begin{pmatrix}
  p^{13}_{00}\\
  p^{13}_{01}\\
  p^{13}_{10}\\
  p^{13}_{11}
 \end{pmatrix}.
\end{equation}
For an arbitrary choice of the quantities $p^{12}_{00}, \ldots,
p^{12}_{11}, p^{13}_{00}, \ldots, p^{13}_{11}$, there is no reason
that these two procedures should yield the same answers for $p^1_0$
and $p^1_1$.  If one requires that they do yield the same answer, then
one must impose consistency conditions.   In our example, these
conditions are as follows:
\begin{eqnarray}
 p^{12}_{00} + p^{12}_{01} &=&
  p^{13}_{00} + p^{13}_{01}\\
 p^{12}_{10} + p^{12}_{11} &=&
  p^{13}_{10} + p^{13}_{11}
\end{eqnarray}

More generally, given a hypergraph $\mathcal{G}$, we will be
interested in two types of consistency conditions.  We will say that a
collection of probabilities associated to a hypergraph is
\emph{locally consistent} if, whenever two hyperedges share a subset in
common, the probabilities for that subset obtained by marginalizing
the probabilities associated to one of the hyperedges will agree with
those obtained by marginalizing the probabilities associated to the
other hypedge.  In our example above, there were only two hypedges
present, so the conditions we exhibited constitute the entirety of the
local consistency conditions for that hypergraph.  We will denote the
set of all locally consistent probability distribution associated to a
hypregraph $\mathcal{G}$ as $\mathbb{L}(\mathcal{G})$.

We will say that a collection of probabilities associated to a
hypergraph is \emph{globally consistent} if there exists a joint
probability distribution on the totality of variables associated to
the hypergraph such that the probabilities associated to any hyperedge
are marginals of that joint distribution.  In terms of our example,
that would mean that there exist probabilities $p^{123}_{000},
p^{123}_{001}, \ldots, p^{123}_{111}$ such that the following
conditions hold:
\begin{equation}
 \begin{pmatrix}
  p^{12}_{00}\\
  p^{12}_{01}\\
  p^{12}_{10}\\
  p^{12}_{11}\\
  p^{13}_{00}\\
  p^{13}_{01}\\
  p^{13}_{10}\\
  p^{13}_{11}
 \end{pmatrix} =
 \begin{pmatrix}
  1 & 1 & 0 & 0 & 0 & 0 & 0 & 0\\
  0 & 0 & 1 & 1 & 0 & 0 & 0 & 0\\
  0 & 0 & 0 & 0 & 1 & 1 & 0 & 0\\
  0 & 0 & 0 & 0 & 0 & 0 & 1 & 1\\
  1 & 0 & 1 & 0 & 0 & 0 & 0 & 0\\
  0 & 1 & 0 & 1 & 0 & 0 & 0 & 0\\
  0 & 0 & 0 & 0 & 1 & 0 & 1 & 0\\
  0 & 0 & 0 & 0 & 0 & 1 & 0 & 1
 \end{pmatrix}
 \begin{pmatrix}
  p^{123}_{000}\\
  p^{123}_{001}\\
  p^{123}_{010}\\
  p^{123}_{011}\\
  p^{123}_{100}\\
  p^{123}_{101}\\
  p^{123}_{110}\\
  p^{123}_{111}\\
 \end{pmatrix}.
\end{equation}
We will denote the set of all globally consistent probability
distribution associated to a hypregraph $\mathcal{G}$ as
$\mathbb{M}(\mathcal{G})$.

Because marginalizing from a set of random variables to a smaller set
of variables can be accomplished by first marginalizing to an
intermediate set and then marginalizing form the intermediate set down
to the smaller set, it follows that global consistency implies local
consistency.  We will now see what conditions are needed in
addition to local consistency to ensure global consistency.

As in our example, we can express marginalization from the set $L$ of
all variables down to a hypergraph $\mathcal{G}$ in the form $v = Gx$
where $x$ is a vector whose components are probabilities associated to
$L$, $v$ is a vector whose components are probabilities associated to
$\mathcal {G}$, and $G$ is a suitable matrix.  The consistency
conditions can be expressed in terms of the fundamental spaces (kernel
and cokernel) associated to this matrix.  In order for a vector $v$ to
be expressible as $Gx$ for some $x$, we must satisfy the condition
that $v\cdot u = 0$ for all $u \in \mathrm{coker} (G)$.  In our
example, the cokernel of the matrix is spanned by the following two
row vectors:
\begin{eqnarray}
 &\begin{pmatrix}
  1 & 1 & 0 & 0 & -1 & -1 & 0 & 0
 \end{pmatrix}\\
 &\begin{pmatrix}
 0 & 0 & 1 & 1 & 0 & 0 & -1 & -1
 \end{pmatrix}
\end{eqnarray}
This leads to the conditions
\begin{eqnarray}
 p^{12}_{00} + p^{12}_{01} - p^{13}_{00} - p^{13}_{01} &=& 0 \\
 p^{12}_{10} + p^{12}_{11} - p^{13}_{10} - p^{13}_{11} &=& 0 .
\end{eqnarray}
Note that these are precisely the local consistency conditions which we
exhibited earlier.  It can be shown that the condition that $u \cdot v
= 0$ for all $u \in \mathrm{coker} (G)$ will always be exactly the
local consistency conditions.

To obtain the global consistency conditions, we note that, if $v =
Gx$, then we also have $v = Gy$ for any vector $y$ such that $x-y$
lies in the kernel of $G$.  Choose a subspace $T$ of column vectors
which is transverse to $\mathrm{ker}(G)$ such that the union of $T$
and $\mathrm{ker}(G)$ span the column space.  Then the equation $v =
Gx$ has a unique solution if we restrict $x$ to lie in $T$.  In order
for a column vector to represent a legitimate probability
distribution, its components must all be non-negative.  Hence, we
conclude that $v$ being globally consistent is equivalent to the
following system of equations and inequalities having a solution:
% \begin{eqnarray}
%  v &=& Gx \\
%  x &\in& T \\
%  x - y &\in& \mathrm{ker}(G) \\
%  y &\ge& 0
% \end{eqnarray}
\begin{equation}
\begin{aligned}\label{eq:globalconsistencyconditions}
 v &= Gx \\
 x &\in T \\
 x - y &\in \mathrm{ker}(G) \\
 y &\ge 0
\end{aligned}
\end{equation}

By using a method, such as Fourier-Motzkin elimination, to remove redundant inequalities, one can eliminate the quantities $x$ and $y$ from this
system to obtain inequallities involving only the components of $v$.
These are the global consistency conditions.

In our example, $\mathrm{ker}(G)$ is spanned by the folllowing two
column vectors:
\begin{equation}
 \begin{pmatrix}
 1 \\ -1 \\ -1 \\ 1 \\ 0 \\ 0 \\ 0 \\ 0
 \end{pmatrix}
 \begin{pmatrix}
 0 \\ 0 \\ 0 \\ 0 \\ 1 \\ -1 \\ -1 \\ 1
 \end{pmatrix}
\end{equation}
As our transverse space $T$, we will choose the space spanned by the
following basis:
\begin{equation}
 \begin{pmatrix}
  1 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0
 \end{pmatrix}
 \begin{pmatrix}
  0 \\ 1 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0
 \end{pmatrix}
 \begin{pmatrix}
  0 \\ 0 \\ 1 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0
 \end{pmatrix}
 \begin{pmatrix}
  0 \\ 0 \\ 0 \\ 0 \\ 1 \\ 0 \\ 0 \\ 0
 \end{pmatrix}
 \begin{pmatrix}
  0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 1 \\ 0 \\ 0
 \end{pmatrix}
 \begin{pmatrix}
  0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 1 \\ 0
 \end{pmatrix}
\end{equation}
With this choice, the condition $x \in T$ reduces to the equations
$x_4 = x_8 = 0$.  The conditions $x - y \in \mathrm{ker}(G)$ then
become
\begin{eqnarray}
 y_1 - x_1 = x_2 - y_2 = x_3 - y_3 &=& y_4 \\
 y_5 - x_5 = x_6 - y_6 = x_7 - y_7 &=& y_8
\end{eqnarray}
If we solve these for the $x$'s, substitute the result into the
equation $v = Gx$ and eliminate the y's between the resulting
equations and the inequalities $y \ge 0$, we find the conditions $v
\ge 0$.  This, of course, is just the condition that the probabilities
be positive.  Thus, for the case of this simple hypergraph, local
consistency suffices to ensure global consistency.  In \ref{sec:inconsistency},
we will see that this is not always the case and that the
inequalities obtained by elimination impose more conditions on the
probabilities than just positivity.

\section{Example of unsatisfiable constraints}\label{sec:inconsistency}
We will now exemplify equations and inequalities that need to be satisfied in order to guarantee the consistency conditions for the simple case of three genes that form the simplest nontrivial cycle where inconsistency may arise. Suppose that $L = \{l_1,l_2,l_3\}$, $P = \{0,1\}$, $\mathcal{G} = \{\{l_1,l_2\},\{l_2,l_3\},\{l_3,l_1\}\}$.
% We define probabilities associated to the gene network-phenotype maps for any set $S$ as $p^S_{\vec{j}}= d_{S}(e^S_{\vec{j}})$. This results in $\mathcal{D}(\mathcal{E}(L)) = \Delta_7 \subset \mathbb{R}^8 $ and $\mathcal{D}(\mathcal{E}(\mathcal{G})) = \Delta^{\oplus 3}_3 \subset \mathbb{R}^{12}$,
% which give the inequalities defining $\mathbb{L}(\mathcal{G})$ and $\mathbb{M}(\mathcal{G})$ when substituted into \ref{eq:localpolytope} and \ref{eq:globalpolytope} with the appropriate $\mathbf{G}$ given by \ref{eq:margmat}.

% An example of normalized contingency tables that correspond to this model are presented in \ref{fig:inconsistentthreecycle}A.
% One local consistency condition in terms of \ref{eq:sheafcond} specifies that for $e_{\{l_1\}} \in \mathcal{E}(\{l_1\})$, which assigns a particular phenotype to the genotype $\{l_1\}$ rather than to an entire gene regulatory network module like $O$ or $O'$, that
% \begin{eqnarray}\label{eq:sheafprob}
% \dist (\expr( \{ l_1 \} \subset O ) )(d_O)(e_{\{l_1\}}) = \sum_{\substack{ e \in \mathcal{E}(O), \\  e|l_1=e_{\{l_1\}}} } d_O(e) \,\, = \sum_{\substack{ e' \in \mathcal{E}(O'), \\ e'|l_1=e_{\{l_1\}}} } d_{O'}(e') = \dist (\expr( \{ l_1 \} \subset O' ) )(d_{O'})(e_{\{l_1\}})
% \end{eqnarray}
% This condition means that the probability for gene $l_1$ to be associated to the phenotype given by $e_{\{l_1\}}$ is equivalent in case we marginalize over all the other genes contained in the gene regulatory network modules of which $l_1$ is a component.  \ref{eq:sheafprob} reduces to two equations corresponding to the case when $e_{l_1}$ is the map which sends $l_1$ to $0$ and the case when $e_{l_1}$ sends $l_1$ to $1$. If we do likewise with $l_2$ and $l_3$ in place of $l_1$ we obtain the set of local consistency conditions:
This condition means that the probability for the variable $l_1$ to be associated to a given state is equivalent in case we marginalize over all the other variables contained in the biological network modules of which $l_1$ is a component.  Mathematically, this reduces to two equations corresponding to the case when the state of $l_1$ is $0$ or $1$. If we do likewise with $l_2$ and $l_3$ in place of $l_1$ we obtain the set of local consistency conditions:
\begin{equation}
\begin{aligned}\label{eq:localconsistencythreegenes}
 p^{12}_{00} + p^{12}_{01} &= p^{1}_0 = p^{13}_{00} + p^{13}_{01}, &
 p^{12}_{00} + p^{12}_{10} &= p^{2}_0 = p^{23}_{00} + p^{23}_{01}, &
 p^{13}_{00} + p^{13}_{10} &= p^{3}_0 = p^{23}_{00} + p^{23}_{10},\\
 p^{12}_{10} + p^{12}_{11} &= p^{1}_1 = p^{13}_{10} + p^{13}_{11}, &
 p^{12}_{01} + p^{12}_{11} &= p^{2}_1 = p^{23}_{10} + p^{23}_{11}, &
 p^{13}_{01} + p^{13}_{11} &= p^{3}_1 = p^{23}_{01} + p^{23}_{11}.
 \end{aligned}
 \end{equation}
These result from applying the method outlined in \ref{sec:compatibilityofgpms} to enumerate all local consistency conditions.
% to what would result from using the cokernel of $\mathbf{G}$ in \ref{eq:localpolytope}.
Using the local consistency conditions for our example we can derive a set of inequalities that determine $\mathbb{L}(\mathcal{G})$
\begin{equation}
\begin{aligned}\label{eq:threecycinequalities}
p^{12}_{00} &= 1 + p^{12}_{11} - p^{23}_{10} - p^{23}_{11} - p^{13}_{10} - p^{13}_{11} \geq 0, \\
p^{12}_{01} &= -p^{12}_{11} + p^{23}_{10} + p^{23}_{11} \geq 0,\\
p^{12}_{10} &= -p^{12}_{11} + p^{13}_{10} + p^{13}_{11} \geq 0,\\
p^{23}_{00} &= 1-p^{23}_{10} - p^{13}_{01} - p^{13}_{11} \geq 0,\\
p^{23}_{01} &= -p^{23}_{11} + p^{13}_{01} + p^{13}_{11} \geq 0,\\
p^{31}_{00} &= 1-p^{13}_{10} - p^{13}_{01} - p^{13}_{11} \geq 0,
\end{aligned}
\end{equation}
combined with the trivial inequalities that force all probabilities to be nonnegative. Substituting the numbers from \ref{fig:inconsistentthreecycle}A into \ref{eq:threecycinequalities}, demonstrates that the local conditions are satisfied.

% The global consistency conditions form an underdetermined system of linear equations for the putative global distribution $d_{L}$ so their solution will assume the form of a linear subspace as demonstrated in \ref{eq:globalpolytope}.  In our example where this linear subspace corresponds to the image of $\Delta_7$ in $\Delta_3^{\oplus 3}$ under the marginalization map, following \ref{eq:globalpolytope} we can choose $v=p^0_{\vec{i}}$, $y=p^L_{\vec{i}}$, and $T$ by fixing $p^{123}_{000}=0$ we get the following by eliminating $x$ from the equations determined by the conditions $v=Gx$, $x \in T$, $x-y \in ker \mathbf{G}$:
The global consistency conditions form an underdetermined system of linear equations for the putative global distribution so their solution will assume the form of a linear subspace.  The following equations arise as a result of eliminating $x$ from the equations determined by the conditions $v=Gx$, $x \in T$, $x-y \in ker \mathbf{G}$:
\begin{equation}
\begin{aligned}\label{eq:globalpositivityeqs}
p^{123}_{001} &= p^{12}_{00} - p^{123}_{000} \\
p^{123}_{010} &= p^{13}_{00} - p^{123}_{000} \\
p^{123}_{100} &= p^{23}_{00} - p^{123}_{000} \\
p^{123}_{110} &= p^{23}_{10} - p^{13}_{00} + p^{123}_{000} \\
p^{123}_{011} &= p^{13}_{01} - p^{12}_{00} + p^{123}_{000} \\
p^{123}_{101} &= p^{12}_{10} - p^{23}_{00} + p^{123}_{000} \\
p^{123}_{111} &= 1 - p^{12}_{00} - p^{13}_{00} - p^{23}_{00} - p^{123}_{000}
\end{aligned}
\end{equation}
% The remaining condition $y \in \mathcal{D}(\mathcal{E}(L))$ from \ref{eq:globalconsistencyconditions} states that all the probabilities $p^{123}_{ijk}$ must be positive numbers, which is only possible if the putative marginals satisfy suitable inequalities given by
The remaining condition $y \geq 0$ from \ref{eq:globalconsistencyconditions} states that all the probabilities $p^{123}_{ijk}$ must be positive numbers, which is only possible if the putative marginals satisfy suitable inequalities given by
\begin{equation}
\begin{aligned}\label{eq:globalpositivityineqs}
p^{123}_{000} &\geq min(p^{12}_{00},\, p^{13}_{00},p^{23}_{00},\, 1 - p^{12}_{00} - p^{13}_{00} - p^{23}_{00}),\\
 p^{123}_{000} &\geq max(0,\, p^{13}_{00}-p^{23}_{10},\, p^{12}_{00}-p^{13}_{01},\, p^{23}_{00}-p^{12}_{10}).
\end{aligned}
\end{equation}
A minimal set of inequalities is then expressed by substituting the equalities from \ref{eq:threecycinequalities} into the inequalities determined by \ref{eq:globalpositivityineqs} and eliminating redundancies resulting in
\begin{equation}
\begin{aligned}\label{eq:threecycbooleinequalities}
p^{12}_{11} - p^{23}_{11} + p^{13}_{01} \geq 0, \\
1 + p^{12}_{11} - p^{23}_{10} - p^{13}_{10} - p^{13}_{01} - p^{13}_{11} \geq 0, \\
-p^{12}_{11} + p^{23}_{10} + p^{13}_{11} \geq 0, \\
-p^{12}_{11} + p^{23}_{11} + p^{13}_{10} \geq 0.
\end{aligned}
\end{equation}
The inequalities from \ref{eq:threecycinequalities} and \ref{eq:threecycbooleinequalities} combined with the nonnegativity inequalities together determine the global polytope $\mathbb{M}(\mathcal{G})$. For the example given in \ref{fig:inconsistentthreecycle}A, the first of the inequalities in \ref{eq:threecycbooleinequalities} is demonstrated to be unsatisfied in \ref{eq:threecycboolenumbers}
\begin{equation}
\begin{aligned}\label{eq:threecycboolenumbers}
0.1 - 0.4 + 0.1 \not\geq 0, \\
1 + 0.1 - 0.1 - 0.1 - 0.1 - 0.4 \geq 0, \\
-0.1 + 0.1 + 0.4 \geq 0, \\
-0.1 + 0.4 + 0.1 \geq 0.
\end{aligned}
\end{equation}
This indicates that data consistent with \ref{fig:inconsistentthreecycle}A could not derive from the network depicted there.
% \subsection{Example of apparent satisfaction of unsatisfiable constraints}\label{sec:apparentinconsistency}
% The inequalities defining $\mathbb{M}(\mathcal{G})$ were derived under the assumption that the two-element probabilities were obtained by mariginalizing a three-element distribution.  If some other procedure, such as conditionalization, is used to obtain them instead, these inequalities need not apply. For example, suppose now that $L = \{l_1,l_2,l_3 \}$, $P = \{0,1,2\}$, $\mathcal{G} = \{\{l_1,l_2\},\{l_2,l_3\},\{l_3,l_1\}\}$ where we have simply added an element to $P$ relative to the example described above. In the previous example the marginal maps were given by $\dist (\expr (O \subset L))$ with one for each $O \in \mathcal{G}$. If we combine these marginal maps with conditioning on one out of the three genes being in state two and each of the other two being in states zero or one, then we have instead $\dist (\pi_1), \dist (\pi_2), \dist (\pi_3)$ where $\pi_1 = \expr (\{l_1,l_2\} \subset L)|\{ e^{123}_{ij2} \mid i,j \in \{ 0,1 \} \},\; \pi_2 = \expr (\{l_2,l_3\} \subset L)|\{ e^{123}_{2ij} \mid i,j \in \{ 0,1 \} \},\; \pi_3 = \expr (\{l_3,l_1\} \subset L)|\{ e^{123}_{i2j} \mid i,j \in \{ 0,1 \} \}$. In this case, if we have the following assignment of probabilities for a distribution $d$
% \begin{equation}\label{eq:condprobs}
% \begin{aligned}
% p^{123}_{002} &= 1/30 & p^{123}_{020} &= 2/15 & p^{123}_{200} &= 2/15\\
% p^{123}_{012} &= 2/15 & p^{123}_{021} &= 1/30 & p^{123}_{201} &= 1/30\\
% p^{123}_{102} &= 2/15 & p^{123}_{120} &= 1/30 & p^{123}_{210} &= 1/30\\
% p^{123}_{112} &= 1/30 & p^{123}_{121} &= 2/15 & p^{123}_{211} &= 2/15
% \end{aligned}
% \end{equation}
% with all other probabilities being zero, then $\dist (\pi_1)(d), \dist (\pi_2)(d), \dist (\pi_3)(d)$ are equivalent to the probability tables in \ref{fig:inconsistentthreecycle}A, which as shown in \ref{sec:inconsistency}, could not be achieved by marginalization alone. For example, given that $dom(\pi_1) = \{ e^{123}_{002}, e^{123}_{012}, e^{123}_{102}, e^{123}_{112} \}$ then $d(dom(\pi_1)) = \frac{1}{30} + \frac{2}{30} + \frac{2}{30} + \frac{1}{30} = \frac{1}{3}$. Substituting this factor and the fact that ${\pi_1}^{-1}(e^{12}_{ij}) = e^{123}_{ij2}$ into \ref{eq:distfunctor}
% $$
% p^{12}_{ij} = p(l_1 = i , l_2 = j \mid l_3 = 2) = \frac{p^{123}_{ij2}}{p^{123}_{002}+p^{123}_{012}+p^{123}_{102}+p^{123}_{112}} = 3 p^{123}_{ij2},
% $$
% then renormalizes probabilities resulting in $p^{12}_{00} = 0.1, p^{12}_{01} = 0.4, p^{12}_{10} = 0.4, p^{12}_{11} = 0.1$ along with the analogs for $p^{23}_{ij}$ and $p^{13}_{ij}$, which are precisely equivalent to what appears in \ref{fig:inconsistentthreecycle}A as suggested above.

% If constraints consistent with those of \ref{fig:inconsistentthreecycle}A are placed on the given network, either the network must add another gene in order to satisfy them directly or the network context imposing those constraints must coarse-grain the network in a suitable way. In what follows, we argue that the former is much more plausible than the latter. This ultimately suggests conditions in which cycle breakage via gene duplication may be selected for to relieve inconsistent constraints that can arise when cycles are present.
